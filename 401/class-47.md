# Ethics

[The code Iâ€™m still ashamed of](https://www.freecodecamp.org/news/the-code-im-still-ashamed-of-e4c021dff55e/) was a pretty horrifying account of someone who was a small piece in a larger move to sell drugs from one particular company, with a questionable approach. It's a really tricky place to be in because the author was just 'doing a job' but at the same time bears some individual responsibility for their actions. There's not a clear action in the moment, but in retrospect one could make an argument for not doing this particular job, which basically pushed a drug onto teen girls, which had some rough side effects and could be a major contributor in those girls committing suicide. People tend to project themselves into situations that they hear about, like this one, and then make some moral judgement on what they themselves would do if they were in the author's shoees, but I don't think thats a fair assessment. Until you are *actually* in a moral dillema, you don't actually know how you will act.

[Self Driving Car Ethics](https://www.freep.com/story/money/cars/2017/11/21/self-driving-cars-ethics/804805001/) and [Ethical dilemma of self driving cars](https://www.theglobeandmail.com/globe-drive/culture/technology/the-ethical-dilemmas-of-self-drivingcars/article37803470/) both reference some strong societal morality issues that come along with self-driving cars. As we all know self-driving cars are heralded as the future of the roads, and will save lives. At the same time they will be put into no-win scenarios, and those scenarios *have* to be considered beforehand. So how to you, as a developer of the car decide how to manage those situations? Like the classic trolley problem, where you can actively kill a person to save multiple others. Or non-actively save a person, at the cost of non-actively allowing others to die. These are age old questions...

[<--- Back](../README.md)
